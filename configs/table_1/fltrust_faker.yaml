# FLTrust - Faker Attack
experiment_name: "fltrust_faker"
random_seed: 42
dataset:
  type: "mnist"
  data_dir: "./data"
  num_classes: 10
  partition_type: "iid"
model:
  type: "lenet"
  num_classes: 10
federated_learning:
  num_rounds: 30
  num_clients: 100
  client_participation_rate: 0.2  # increased to get more malicious clients per round
  learning_rate: 0.01
  local_epochs: 2
  batch_size: 32
server:
  data_size: 500
attack:
  type: "faker"
  num_malicious_clients: 100  # 70% malicious - testing breaking point
  params:
    target_defense: "fltrust"
    optimization_mode: "constrained"
    num_groups: 100                   # more groups = finer control for partial layer flipping
    optimization_iterations: 50
    min_cosine_margin: 0.05           # min viable similarity (barely legal)
    alpha_min: -10.0                  # allow negative alphas (layer flipping)
    alpha_max: 10.0                   # max amplification per group
    # we force negative initialization to explore anti-gradient space
    # otpinks: "alternating_negative", "all_negative", "random_negative", "gradient_aware"
    init_strategy: "alternating_negative"
defense:
  type: "fltrust"
  params:
    server_epochs: 5
    server_lr: 0.01
    cosine_threshold: 0.0
    use_relu: true
    clip_updates: false
    use_statistical_filtering: true
    std_threshold: 1.5